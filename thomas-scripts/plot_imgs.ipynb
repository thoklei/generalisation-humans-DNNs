{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the wrongly classified images in nice overview plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from show_failed_imgs import *\n",
    "from download_imgs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../raw-data/humans/colour-experiment/colour-experiment_subject-01_session_1.csv\"\n",
    "img_location = \"/mnt/qb/work/bethge/tklein16/brains_vs_dnns_out/wrong_imgs\"\n",
    "imagenet_path = \"/imagenet/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# get list of wrong images\n",
    "wrongs = get_wrong_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all images where subjects gave wrong responses\n",
    "download_wrong_images([img for _, _, img in wrongs], img_location, imagenet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make overview plot of wrongly labelled images\n",
    "\n",
    "def make_overview_plot():\n",
    "    ncols = 8\n",
    "    nrows = int(np.ceil(len(wrongs) / ncols))\n",
    "\n",
    "    print(f\"Subject got {len(wrongs)} of {len(df)} images wrong.\")\n",
    "\n",
    "    scale = 2.5\n",
    "    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(ncols*scale, nrows*scale))\n",
    "    ax = ax.flatten()\n",
    "    for idx, (response, label, img_path) in enumerate(wrongs):\n",
    "        ax[idx].imshow(get_img(img_location, img_path))\n",
    "        ax[idx].set_xlabel(f\"{response} / {label}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusions.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "make_overview_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix():\n",
    "    \n",
    "    classes= [\n",
    "        'airplane',\n",
    "        'bear',\n",
    "        'bicycle',\n",
    "        'bird',\n",
    "        'boat',\n",
    "        'bottle',\n",
    "        'car',\n",
    "        'cat',\n",
    "        'chair',\n",
    "        'clock',\n",
    "        'dog',\n",
    "        'elephant',\n",
    "        'keyboard',\n",
    "        'knife',\n",
    "        'oven',\n",
    "        'truck',\n",
    "        'na' # no response\n",
    "    ]\n",
    "    class_numbers = {cl:num for num, cl in enumerate(classes)}\n",
    "\n",
    "    conf = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "    for response, label, _ in wrongs:\n",
    "        conf[class_numbers[response], class_numbers[label]] += 1\n",
    "\n",
    "    # cut off na-column\n",
    "    conf = conf[:,:-1]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(conf)\n",
    "    plt.colorbar()\n",
    "    plt.ylabel(\"People said...\")\n",
    "    plt.xticks(np.arange(0,len(classes)-1), classes[:-1], rotation=45) # ignore na\n",
    "    plt.xlabel(\"When it was...\")\n",
    "    plt.yticks(np.arange(0,len(classes)), classes)\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "make_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypotheses\n",
    "\n",
    "- cropping sometimes cuts off the relevant parts, did Robert crop?\n",
    "- people have a preference for saying cat / dog instead of bear, because they are much more familiar with pets? (saying cat instead of bear seems to be big wild cats like lions, mostly)\n",
    "- cats and dogs get confused because sometimes it's just hard to tell...\n",
    "- oven and knife get confused a lot, seemingly because many pictures of ovens are just weird-looking (grills and stuff like that)\n",
    "- frequently, the label is unclear, because both objects are in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('standard')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47ba74dac42e143af704e1f99c5fb09010a5cdca8bd83b0d1f595774210bb551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
